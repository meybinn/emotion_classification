import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score,classification_report
from sklearn.svm import SVC
from sklearn.calibration import CalibratedClassifierCV

# 1. ë°ì´í„°
f_df = pd.read_csv('preprocessed_emotion_hr_features_by_video.csv')
s_df = pd.read_csv('features_for_FT_classification.csv')#[['patient_id','mbti_tf']]

df = f_df.merge(s_df,on='patient_id',how='left')
print(df.columns)   

# 2. ë¼ë²¨ ë° íŠ¹ì§• ì •ì˜
df['label'] = df["emotion"].map({"sad":0, "angry": 1})
features = [
    "mean_hr","std_hr","range_hr", #"min_hr","max_hr",
    # "delta_mean","delta_std","delta_range",
    # "mean_hr_sad","mean_hr_angry"
]

x = df[features].values    # ëª¨ë¸ ì…ë ¥
y = df["label"].values     # ì •ë‹µ (sad=0 angry=1)
tf = df["mbti_tf"].values  # tf êµ¬ë¶„

# 3. ë°ì´í„° ë¶„í•  -> train=ëª¨ë¸ í•™ìŠµ val=ì„ê³„ì  test=ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
#(1) ëœë¤ ë¶„í•  -> t/f ë¹„ìœ¨ ê· í˜•ìˆê²Œ X
# from sklearn.model_selection import GroupShuffleSplit
# from imblearn.over_sampling import SMOTE

# gss = GroupShuffleSplit(n_splits=1,test_size=0.2, random_state=42)
# idx_train, idx_temp = next(gss.split(df,groups=df["patient_id"]))
# train = df.iloc[idx_train]; temp=df.iloc[idx_temp]

# gss2 = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=43)
# idx_val, idx_test = next(gss2.split(temp, groups=temp["patient_id"]))
# val = temp.iloc[idx_val]; test = temp.iloc[idx_test]

# x_train_bin_raw = train[features].values
# y_train_bin = train["label"].values

from sklearn.model_selection import GroupShuffleSplit, StratifiedGroupKFold
from imblearn.over_sampling import SMOTE

# StratifiedGroupKFoldë¡œ train/val/test ë¶„í• 
sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)

# ì²« ë²ˆì§¸ splitì„ train/valë¡œ ì‚¬ìš©
for train_idx, val_idx in sgkf.split(df, df["label"], groups=df["patient_id"]):
    train = df.iloc[train_idx]
    val = df.iloc[val_idx]
    break

# train+valë¡œë¶€í„° test ë”°ë¡œ ì¶”ì¶œ (T/F ê·¸ë£¹ì´ ë„ˆë¬´ ì‘ìœ¼ë‹ˆê¹Œ testë§Œ ë”°ë¡œ ë¶„ë¦¬)
gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=43)
idx_temp, idx_test = next(gss.split(df, groups=df["patient_id"]))
test = df.iloc[idx_test]

# feature/label ë¶„ë¦¬
x_train_bin_raw = train[features].values
y_train_bin = train["label"].values

# ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler().fit(train[features])
x_train_bin_scaled = scaler.transform(x_train_bin_raw)
x_val = scaler.transform(val[features])
x_test = scaler.transform(test[features])

# SMOTE ì ìš©
# smote = SMOTE(random_state=42)
# x_train_resampled, y_train_resampled = smote.fit_resample(x_train_bin_scaled, y_train_bin)

# ì˜ˆì¸¡ (3í´ë˜ìŠ¤ ë¶„ë¥˜ í•¨ìˆ˜)
def predict_3class(p_ang, th_ang, th_sad):
    p_sad = 1 - p_ang
    pred = np.zeros(len(p_ang), dtype=int)  # default: normal
    pred[p_ang >= th_ang] = 2               # angry
    pred[(p_ang < th_ang) & (p_sad >= th_sad)] = 1  # sad
    return pred

# def sweep_for_group(p, y, mask):
#     best = {"th_ang": None, "th_sad": None, "score": -1}
#     for ta in np.arange(0.3, 0.71, 0.01):
#         for ts in np.arange(0.3, 0.71, 0.01):
#             pred = predict_3class(p[mask], ta, ts)
#             f1 = f1_score(y[mask], pred, average="macro", labels=[1, 2])
#             if f1 > best["score"]:
#                 best = {"th_ang": round(ta, 2), "th_sad": round(ts, 2), "score": round(f1, 3)}
#     return best

# def sweep_for_group(p, y, mask):
#     best = {"th_ang": None, "th_sad": None, "score": -1.0}
#     # grid = np.arange(0.30, 0.81, 0.01)  # íƒìƒ‰ ë²”ìœ„ ë„“í˜
#     grid = np.arange(0.45, 0.9, 0.02) # ë” ë„“í˜...
#     for ta in grid:
#         for ts in grid:
#             # 3í´ë˜ìŠ¤ ì˜ˆì¸¡
#             pred3 = predict_3class(p[mask], ta, ts)
#             # NORMAL/SAD=0, ANGRY=1
#             pred2 = (pred3 == 2).astype(int)
#             f1 = f1_score(y[mask], pred2, average="binary", zero_division=0)
#             if f1 > best["score"]:
#                 best = {"th_ang": round(ta, 2), "th_sad": round(ts, 2), "score": round(f1, 3)}
#     return best

def sweep_for_group(p, y, mask,
                    grid=np.arange(0.45, 0.90, 0.02),
                    normal_band_margin=0.05):  # NORMAL ìµœì†Œ í­(~5%)

    # ëª¨ë‘ ë„˜íŒŒì´ë¡œ í†µì¼(ìœ„ì¹˜ ì¸ë±ì‹±)
    p = np.asarray(p)
    y = np.asarray(y)
    mask = np.asarray(mask, dtype=bool)
    idx = np.flatnonzero(mask)
    if idx.size == 0:
        return None

    best = {"th_ang": None, "th_sad": None, "score": -1.0}

    for ta in grid:
        for ts in grid:
            # NORMAL ë°´ë“œ ê°•ì œ: th_ang > 1 - th_sad + margin
            if ta <= (1 - ts + normal_band_margin):
                continue

            pred3 = predict_3class(p[idx], ta, ts)   # 0=N,1=S,2=A
            pred2 = (pred3 == 2).astype(int)         # A vs (S/N)
            f1 = f1_score(y[idx], pred2, average="binary", zero_division=0)

            if f1 > best["score"]:
                best = {"th_ang": float(round(ta, 2)),
                        "th_sad": float(round(ts, 2)),
                        "score": float(round(f1, 3))}
    return best


val_mbti = val["mbti_tf"].astype(str).str.strip().str.lower()
print("VAL T/F counts:\n", val_mbti.value_counts())
print("\nVAL label counts (0=SAD,1=ANGRY):\n", val["label"].value_counts())
print("\nVAL crosstab:\n", pd.crosstab(val_mbti, val["label"]))

# SVM ëª¨ë¸
# (1) ê¸°ë³¸ ëª¨ë¸
clf_svm = SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=42)
clf_svm.fit(x_train_bin_scaled, y_train_bin)

# ì„ê³„ì  - ê¸°ë³¸ ëª¨ë¸ì— ë§ì¶°ì„œ 
p_val_svm = clf_svm.predict_proba(x_val)[:, 1]  # angry í™•ë¥ 

# SVM + í™•ë¥  ë³´ì •(Platt)
# base_svm = SVC(kernel='rbf', class_weight='balanced', probability=False, random_state=42)
# clf = CalibratedClassifierCV(base_svm, method="sigmoid", cv=3)  # ë˜ëŠ” "isotonic"
# clf.fit(x_train_bin_scaled, y_train_bin)

# # ê²€ì¦/í…ŒìŠ¤íŠ¸ í™•ë¥ 
# p_val_svm  = clf.predict_proba(x_val)[:, 1]   # angry í™•ë¥ 
# p_test_svm = clf.predict_proba(x_test)[:, 1]

# ê¸°ë³¸ê°’ìœ¼ë¡œ
# maskT = (val["mbti_tf"]=="t")
# maskF = (val["mbti_tf"]=="f")

# í•œ ë²ˆ ì¸ë±ì‹±ëœ ê°’ìœ¼ë¡œ
# maskT = (val_mbti == "t")
# maskF = (val_mbti == "f")

# bestF_svm = sweep_for_group(p_val_svm, val["label"], maskF)
# bestT_svm = sweep_for_group(p_val_svm, val["label"], maskT)

# fê°€ ì¢€ ì ì„ ë•Œ 
# ë§ˆìŠ¤í¬ ë„˜íŒŒì´ë¡œ
val_mbti_norm = val["mbti_tf"].astype(str).str.strip().str.lower()
maskT = (val_mbti_norm == "t").to_numpy()
maskF = (val_mbti_norm == "f").to_numpy()
maskALL = np.ones(len(val), dtype=bool)

# ë„˜íŒŒì´ ë¼ë²¨ ì „ë‹¬
y_val_np = val["label"].to_numpy()

# ì„ê³„ê°’ íƒìƒ‰
bestT_svm = sweep_for_group(p_val_svm, y_val_np, maskT)
bestF_svm = sweep_for_group(p_val_svm, y_val_np, maskF)
bestALL   = sweep_for_group(p_val_svm, y_val_np, maskALL)

# F í´ë°±(í‘œë³¸ ë¶€ì¡±/ìŠ¤ì½”ì–´ ë¶ˆê°€ ì‹œ)
if (maskF.sum() < 4) or (bestF_svm is None) or (bestF_svm.get("score", 0) <= 0):
    bestF_svm = bestALL or bestT_svm

# ---- í…ŒìŠ¤íŠ¸ì…‹ 3í´ë˜ìŠ¤ íŒì • (T/Fë³„ ì„ê³„ê°’ ì ìš©) ----
# 1) mbti_tf ë¬¸ìì—´ ì •ê·œí™” (ê³µë°±/ëŒ€ì†Œë¬¸ì ì•ˆì „)
val_mbti_norm  = val["mbti_tf"].astype(str).str.strip().str.lower()
test_mbti_norm = test["mbti_tf"].astype(str).str.strip().str.lower()

# 2) í…ŒìŠ¤íŠ¸ì…‹ í™•ë¥ (p_angry) ì‚°ì¶œ
# p_test_svm = clf_svm.predict_proba(x_test)[:, 1]      # angry í™•ë¥ 
p_test_svm = clf_svm.predict_proba(x_test)[:, 1]      # angry í™•ë¥ 
tf_test_str = test_mbti_norm.to_numpy()               # 't' ë˜ëŠ” 'f'
y_true = test["label"].to_numpy().astype(int)         # 0=SAD, 1=ANGRY

# 3) T/Fë³„ (th_ang, th_sad) ì ìš©í•˜ì—¬ 3í´ë˜ìŠ¤ ì˜ˆì¸¡ ìƒì„±
def predict_3class_with_tf(p_ang, tf_group_str, bestT, bestF):
    out = np.zeros(len(p_ang), dtype=int)  # 0=NORMAL
    for i, (p, g) in enumerate(zip(p_ang, tf_group_str)):
        isT = (g == "t")
        th_ang = bestT["th_ang"] if isT else bestF["th_ang"]
        th_sad = bestT["th_sad"] if isT else bestF["th_sad"]
        # êµ¬ê°„ ê·œì¹™: angry ìš°ì„  â†’ sad â†’ ë‚˜ë¨¸ì§€ normal
        out[i] = 2 if p >= th_ang else (1 if (1 - p) >= th_sad else 0)
    return out

pred3_test = predict_3class_with_tf(p_test_svm, tf_test_str, bestT_svm, bestF_svm)

# 4) ë¦¬í¬íŠ¸: ë³¸ì§ˆ 2í´ë˜ìŠ¤ ì„±ëŠ¥(angry vs sad) + NORMAL ë¹„ìœ¨
#    NORMAL(0)ì€ ì˜¤ë‹µìœ¼ë¡œ ì·¨ê¸‰í•˜ì—¬ 2í´ë˜ìŠ¤ë¡œ ì••ì¶•í•´ í‰ê°€
pred2_from3 = (pred3_test == 2).astype(int)   # 2(ANGRY)ë§Œ 1, ë‚˜ë¨¸ì§€(NORMAL/SAD)ëŠ” 0

# ì„ê³„ì  í™•ì¸
print("Best thresholds (T):", bestT_svm)
print("Best thresholds (F):", bestF_svm)

# Fê·¸ë£¹ì—ì„œ í™•ë¥  ë¶„í¬ ë³´ê¸°
p_val = p_val_svm  # ì´ë¯¸ ê³„ì‚°ë¨
print("\nF group size:", (val_mbti=="f").sum())
vals = p_val[val_mbti=="f"]
if len(vals) > 0:
    print("F group p_angry stats:", np.min(vals), np.mean(vals), np.max(vals))
else:
    print("F group p_angry stats: [no samples]")

from sklearn.metrics import classification_report
print("\n[SVM] 2-class report (from 3-class TF-threshold rule):")
print(classification_report(y_true, pred2_from3,
                            labels=[0,1], target_names=["SAD","ANGRY"], zero_division=0))
print("NORMAL rate on test:", float((pred3_test == 0).mean()))

# ì„ê³„ì  ê·¸ë¦¼ìœ¼ë¡œ í™•ì¸ 
# import matplotlib.pyplot as plt
# plt.hist(p_val_svm, bins=10, color='skyblue', edgecolor='black')
# plt.axvline(bestT_svm["th_ang"], color='r', linestyle='--', label='th_ang')
# plt.axvline(1 - bestT_svm["th_sad"], color='b', linestyle='--', label='1 - th_sad')
# plt.title("Validation p_angry distribution")
# plt.legend(); plt.show()

val_mbti = val["mbti_tf"].astype(str).str.strip().str.lower()
p = p_val_svm
print("T p_angry:", np.min(p[val_mbti=="t"]), np.mean(p[val_mbti=="t"]), np.max(p[val_mbti=="t"]))
print("F p_angry:", np.min(p[val_mbti=="f"]), np.mean(p[val_mbti=="f"]), np.max(p[val_mbti=="f"]))

svm_raw = SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=42).fit(x_train_bin_scaled, y_train_bin)
p_raw = svm_raw.predict_proba(x_val)[:,1]
print("RAW prob mean/std:", np.mean(p_raw), np.std(p_raw))
print("CAL prob mean/std:", np.mean(p_val_svm), np.std(p_val_svm))

# === ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ + ì„ê³„ê°’ ìŠ¤ìœ• + 3í´ë˜ìŠ¤ ì ìš© + classification_report ì¶œë ¥ ===
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import LinearSVC
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import classification_report
import numpy as np
import pandas as pd

# (ì „ì œ) ì•„ë˜ ë³€ìˆ˜/í•¨ìˆ˜ë“¤ì´ ê¸°ì¡´ ì½”ë“œ ìƒë‹¨ì—ì„œ ì´ë¯¸ ì¡´ì¬í•œë‹¤ê³  ê°€ì •:

def eval_model(model_name, clf_proba):
    """clf_proba: fit(X,y) ë° predict_proba(X)[:,1] ê°€ëŠ¥í•œ ë¶„ë¥˜ê¸° (ë˜ëŠ” Calibrated wrapper)"""
    print("\n" + "="*80)
    print(f"[{model_name}]")

    # 1) í•™ìŠµ
    clf_proba.fit(x_train_bin_scaled, y_train_bin)

    # 2) ê²€ì¦/í…ŒìŠ¤íŠ¸ í™•ë¥  (angry=1ì˜ í™•ë¥ )
    p_val  = clf_proba.predict_proba(x_val)[:, 1]
    p_test = clf_proba.predict_proba(x_test)[:, 1]

    # 3) T/F ë§ˆìŠ¤í¬ (ë„˜íŒŒì´)
    val_mbti_norm  = val["mbti_tf"].astype(str).str.strip().str.lower()
    test_mbti_norm = test["mbti_tf"].astype(str).str.strip().str.lower()
    maskT = (val_mbti_norm == "t").to_numpy()
    maskF = (val_mbti_norm == "f").to_numpy()
    maskALL = np.ones(len(val), dtype=bool)
    y_val_np = val["label"].to_numpy()

    # 4) ì„ê³„ê°’ ìŠ¤ìœ• (T/F/ì „ì²´) + F í´ë°±
    bestT = sweep_for_group(p_val, y_val_np, maskT)
    bestF = sweep_for_group(p_val, y_val_np, maskF)
    bestALL = sweep_for_group(p_val, y_val_np, maskALL)
    if (maskF.sum() < 4) or (bestF is None) or (bestF.get("score", 0) <= 0):
        bestF = bestALL or bestT

    print("Best thresholds (T):", bestT)
    print("Best thresholds (F):", bestF)

    # 5) í…ŒìŠ¤íŠ¸ì…‹ 3í´ë˜ìŠ¤ ì ìš©
    tf_test_str = test_mbti_norm.to_numpy()              # 't'/'f'
    y_true = test["label"].to_numpy().astype(int)        # 0=SAD, 1=ANGRY

    def predict_3class_with_tf(p_ang, tf_group_str, bestT, bestF):
        out = np.zeros(len(p_ang), dtype=int)  # 0=NORMAL
        for i, (p, g) in enumerate(zip(p_ang, tf_group_str)):
            isT = (g == "t")
            th_ang = bestT["th_ang"] if isT else bestF["th_ang"]
            th_sad = bestT["th_sad"] if isT else bestF["th_sad"]
            # ê·œì¹™: angry ìš°ì„  â†’ sad â†’ ë‚˜ë¨¸ì§€ normal
            out[i] = 2 if p >= th_ang else (1 if (1 - p) >= th_sad else 0)
        return out

    pred3_test = predict_3class_with_tf(p_test, tf_test_str, bestT, bestF)

    # 6) ë¦¬í¬íŠ¸(ëª©ì ì— ë§ëŠ” ì§€í‘œ): 3í´ë˜ìŠ¤ ê·œì¹™ì—ì„œ angry vs (sad/normal) ì´ì§„
    pred2_from3 = (pred3_test == 2).astype(int)
    print("\n[{}] 2-class report (from 3-class TF-threshold rule):".format(model_name))
    print(classification_report(y_true, pred2_from3,
                                labels=[0,1], target_names=["SAD","ANGRY"], zero_division=0))
    print("NORMAL rate on test:", float((pred3_test == 0).mean()))

# ---- í›„ë³´ ëª¨ë¸ë“¤ ----

# 2) Random Forest
clf_rf = RandomForestClassifier(
    n_estimators=300, max_depth=None, min_samples_leaf=1,
    class_weight='balanced', random_state=42
)

# 3) Gradient Boosting
clf_gb = GradientBoostingClassifier(random_state=42)

# ì‹¤í–‰
eval_model("RandomForest",       clf_rf)
eval_model("GradientBoosting",   clf_gb)

# gb ëª¨ë¸ thresholds ì €ì¥
clf_gb.fit(x_train_bin_scaled, y_train_bin)
p_val_gb = clf_gb.predict_proba(x_val)[:, 1]

bestT_gb  = sweep_for_group(p_val_gb, y_val_np, maskT,  normal_band_margin=0.10)
bestF_gb  = sweep_for_group(p_val_gb, y_val_np, maskF,  normal_band_margin=0.10)
bestALL_gb= sweep_for_group(p_val_gb, y_val_np, maskALL,normal_band_margin=0.10)

if (maskF.sum() < 4) or (bestF_gb is None) or (bestF_gb.get("score", 0) <= 0):
    bestF_gb = bestALL_gb or bestT_gb

print("âœ… Gradient Boosting thresholds:")
print("Best thresholds (T):", bestT_gb)
print("Best thresholds (F):", bestF_gb)
print("Best thresholds (ALL):", bestALL_gb)

import json, joblib
thresholds_dict = {
    "T": bestT_gb,
    "F": bestF_gb,
    "ALL": bestALL_gb,
    "use": "TF"
}

with open("thresholds.json", "w", encoding="utf-8") as f:
    json.dump(thresholds_dict, f, indent=4)
print("ğŸ’¾ Gradient Boostingìš© thresholds.json ì €ì¥ ì™„ë£Œ!")

joblib.dump(scaler, "scaler.pkl")
joblib.dump(clf_gb, "model_gb.pkl")
print("ğŸ’¾ ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ!")

# ì–´í”Œë¦¬ì¼€ì´ì…˜ê³¼ ì—°ë™ (API)
# import joblib

#     # í•™ìŠµ ëë‚œ ë’¤ ì¶”ê°€ â†“
# joblib.dump(scaler, "scaler.pkl")
# joblib.dump(clf_svm, "model.pkl")
# joblib.dump(clf_rf, "model_rf.pkl")
# joblib.dump(clf_gb, "model_gb.pkl")
# joblib.dump(
#     {"T": bestT_svm, "F": bestF_svm, "ALL": bestALL, "use": "TF"},
#     "thresholds.pkl"
# )
# print("ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, ì„ê³„ê°’ ì €ì¥ ì™„ë£Œ!")

# ì–´í”Œë¦¬ì¼€ì´ì…˜ê³¼ ì—°ë™ (json)
# import json

# thresholds_dict = {
#     "T": bestT_svm,
#     "F": bestF_svm,
#     "ALL": bestALL,
#     "use": "TF"  # ë˜ëŠ” "ALL"
# }

# with open("thresholds.json", "w", encoding="utf-8") as f:
#     json.dump(thresholds_dict, f, indent=4)
# print("âœ… thresholds.json ì €ì¥ ì™„ë£Œ!")


# ê¸°ë³¸ svm ì´ì§„ ë¶„ë¥˜
# p_test_svm = clf_svm.predict_proba(x_test)[:, 1]
# pred_test_svm = clf_svm.predict(x_test)

# í‰ê°€ ê²°ê³¼
# print(classification_report(test["label"], pred_test_svm, labels=[0, 1], target_names=["SAD", "ANGRY"], zero_division=0))

# print("train size:", len(train), "val size:", len(val), "test size:", len(test))
# print("train F1:", f1_score(y_train_resampled, clf_svm.predict(x_train_resampled), average="macro"))
# print("val F1:",   f1_score(val["label"], clf_svm.predict(x_val), average="macro"))

import joblib
# After training...
joblib.dump(scaler, "scaler.pkl")
joblib.dump(clf_gb, "model_gb.pkl")
